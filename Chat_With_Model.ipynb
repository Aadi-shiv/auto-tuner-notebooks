{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Chat with Your Fine-Tuned Model\n",
    "\n",
    "Use this notebook to have real conversations with your trained AI model!\n",
    "\n",
    "**What you'll need:**\n",
    "- Your trained model in Google Drive (`Finetune_Jobs/models/your-model-name/`)\n",
    "- Free T4 GPU (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
    "\n",
    "**Time:** ~3 minutes to load model, then instant responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 1: Configuration\n",
    "\n",
    "**IMPORTANT:** Update `MODEL_NAME` to match your trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - UPDATE THIS\n",
    "# ============================================================================\n",
    "\n",
    "# Your model name (from training notebook)\n",
    "MODEL_NAME = \"customer-support-bot-v1\"  # ‚Üê CHANGE THIS\n",
    "\n",
    "# Model path in Google Drive\n",
    "MODEL_PATH = f\"/content/drive/MyDrive/Finetune_Jobs/models/{MODEL_NAME}\"\n",
    "\n",
    "# Chat settings\n",
    "TEMPERATURE = 0.7      # 0.0 = deterministic, 1.0 = creative\n",
    "MAX_TOKENS = 256       # Maximum response length\n",
    "TOP_P = 0.9           # Nucleus sampling\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Path: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify model exists\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"‚ùå ERROR: Model not found at {MODEL_PATH}\")\n",
    "    print(f\"\\nAvailable models in Finetune_Jobs/models/:\")\n",
    "    models_dir = \"/content/drive/MyDrive/Finetune_Jobs/models\"\n",
    "    if os.path.exists(models_dir):\n",
    "        for model in os.listdir(models_dir):\n",
    "            print(f\"  - {model}\")\n",
    "    else:\n",
    "        print(\"  (No models folder found)\")\n",
    "    raise FileNotFoundError(f\"Please update MODEL_NAME in Step 1\")\n",
    "\n",
    "print(f\"‚úÖ Drive mounted\")\n",
    "print(f\"‚úÖ Model found: {MODEL_PATH}\")\n",
    "\n",
    "# Show model files\n",
    "print(f\"\\nüìÅ Model files:\")\n",
    "for file in os.listdir(MODEL_PATH):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 3: Install Unsloth (Fast Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth for 2x faster inference\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" \"peft\" \"accelerate\" \"bitsandbytes\"\n",
    "\n",
    "print(\"‚úÖ Unsloth installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4: Load Your Model\n",
    "\n",
    "**This takes ~2-3 minutes. Please wait...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(\"‚è≥ Loading base model (Llama 3 8B)...\")\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Loading your fine-tuned adapter...\")\n",
    "\n",
    "# Load your LoRA adapter\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, MODEL_PATH)\n",
    "\n",
    "# Enable fast inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"‚úÖ Model loaded and ready!\")\n",
    "print(f\"üéØ Using: {MODEL_NAME}\")\n",
    "print(f\"‚ö° Fast inference enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 5: Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history=None, temperature=TEMPERATURE, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"\n",
    "    Chat with your model.\n",
    "    \n",
    "    Args:\n",
    "        message: Your question/message\n",
    "        history: Previous conversation (optional)\n",
    "        temperature: Creativity level (0.0-1.0)\n",
    "        max_tokens: Max response length\n",
    "    \n",
    "    Returns:\n",
    "        Model's response\n",
    "    \"\"\"\n",
    "    # Build conversation history\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Add current message\n",
    "    messages = history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Format for model\n",
    "    try:\n",
    "        # Try using chat template\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "    except:\n",
    "        # Fallback to simple format\n",
    "        prompt = f\"### User:\\n{message}\\n\\n### Assistant:\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids if hasattr(inputs, 'input_ids') else inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"### Assistant:\" in full_response:\n",
    "        response = full_response.split(\"### Assistant:\")[-1].strip()\n",
    "    elif \"assistant\" in full_response.lower():\n",
    "        response = full_response.split(\"assistant\")[-1].strip()\n",
    "    else:\n",
    "        # If no markers, take everything after the input\n",
    "        response = full_response[len(message):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Chat function ready\")\n",
    "print(\"You can now use: chat('your message here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 6: Test Your Model\n",
    "\n",
    "Try a quick test to make sure everything works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test message\n",
    "test_message = \"Hello! Can you help me?\"\n",
    "\n",
    "print(f\"üë§ User: {test_message}\")\n",
    "print(\"\\n‚è≥ Generating response...\\n\")\n",
    "\n",
    "response = chat(test_message)\n",
    "\n",
    "print(f\"ü§ñ {MODEL_NAME}: {response}\")\n",
    "print(\"\\n‚úÖ Model is working! You can now chat below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 7: Interactive Chat\n",
    "\n",
    "**Run this cell and start chatting!**\n",
    "\n",
    "Type your messages and press Enter. Type 'quit' or 'exit' to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"üí¨ Chat with {MODEL_NAME}\")\n",
    "print(\"=\"*80)\n",
    "print(\"Type your message and press Enter. Type 'quit' or 'exit' to stop.\\n\")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_message = input(\"\\nüë§ You: \").strip()\n",
    "    \n",
    "    # Check for exit\n",
    "    if user_message.lower() in ['quit', 'exit', 'bye']:\n",
    "        print(\"\\nüëã Thanks for chatting! Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    if not user_message:\n",
    "        continue\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    response = chat(user_message, history=conversation_history)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Display response\n",
    "    print(f\"\\nü§ñ {MODEL_NAME}: {response}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Response time: {elapsed:.2f}s\")\n",
    "    \n",
    "    # Update conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    # Keep only last 5 exchanges to avoid context overflow\n",
    "    if len(conversation_history) > 10:\n",
    "        conversation_history = conversation_history[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Step 8: Advanced Chat (With Settings)\n",
    "\n",
    "Try different settings to control how your model responds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different temperatures:\n",
    "# - Low (0.1-0.3): More focused, consistent, deterministic\n",
    "# - Medium (0.5-0.7): Balanced\n",
    "# - High (0.8-1.0): More creative, varied, random\n",
    "\n",
    "question = \"What is machine learning?\"\n",
    "\n",
    "print(\"Testing different temperature settings:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0]:\n",
    "    print(f\"\\nüå°Ô∏è  Temperature: {temp}\")\n",
    "    print(f\"üë§ User: {question}\")\n",
    "    response = chat(question, temperature=temp)\n",
    "    print(f\"ü§ñ Bot: {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 9: Batch Questions (Optional)\n",
    "\n",
    "Test multiple questions at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of questions to test\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How can I reset my password?\",\n",
    "    \"Tell me about neural networks\",\n",
    "    \"What's the difference between AI and ML?\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing batch questions\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. üë§ {question}\")\n",
    "    response = chat(question)\n",
    "    print(f\"   ü§ñ {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 10: Save Conversation (Optional)\n",
    "\n",
    "Save your chat history to Google Drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save conversation to file\n",
    "if conversation_history:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_path = f\"/content/drive/MyDrive/Finetune_Jobs/conversations/chat_{timestamp}.json\"\n",
    "    \n",
    "    # Create directory if needed\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Save conversation\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"conversation\": conversation_history\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Conversation saved to: {save_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No conversation to save. Chat first in Step 7!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ All Done!\n",
    "\n",
    "You're now chatting with your real fine-tuned model!\n",
    "\n",
    "**Tips:**\n",
    "- Use Step 7 for natural conversations\n",
    "- Try Step 8 to experiment with temperature settings\n",
    "- Use Step 9 to test multiple questions quickly\n",
    "- Save important conversations with Step 10\n",
    "\n",
    "**Performance:**\n",
    "- First response: ~3-5 seconds (model warmup)\n",
    "- Subsequent responses: ~1-2 seconds\n",
    "- With GPU: Very fast!\n",
    "\n",
    "**Need help?** Check the Auto-Tuner documentation or ask in the community!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}